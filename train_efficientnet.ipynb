{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fde579-c955-4a16-a019-5ab53a8e56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import PIL\n",
    "import tensorflow as tf\n",
    "import pickle5 as pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "off=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa310424-8b15-4830-a20c-cb8d335cd735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ch1_mean, ch1_std = 970.4162, 1051.8454\n",
    "ch2_mean, ch2_std = 1176.249, 1031.262\n",
    "ch3_mean, ch3_std = 1273.2376, 1117.068\n",
    "ch4_mean, ch4_std = 2266.9050, 1241.5509\n",
    "means = [ch1_mean,ch2_mean,ch3_mean,ch4_mean]\n",
    "vari = [ch1_std, ch2_std, ch3_std, ch4_std]\n",
    "x_size, y_size = 230,230\n",
    "def load_features(name):\n",
    "    decoded = name.decode(\"UTF-8\")\n",
    "    if os.path.exists(decoded):\n",
    "        with open(decoded, 'rb') as f:\n",
    "            file = pickle.load(f)\n",
    "            label = tf.strings.split(tf.strings.split(name, '/')[-1], '\\\\')[-2]\n",
    "            if label == 'coal':\n",
    "                label = [1,0,0]\n",
    "            elif label == 'steel':\n",
    "                label = [0,1,0]\n",
    "            else: label = [0,0,1]\n",
    "            if (file[\"B02\"].shape[1]<230 or file[\"B02\"].shape[2]<230): \n",
    "                print(\"oh oh, downloaded patch too smol:\", decoded)\n",
    "            B02, B03, B04, B08 = file['B02'][0][0:x_size,0:x_size], file['B03'][0][0:x_size,0:y_size], file['B04'][0][0:x_size,0:y_size],file['B08'][0][0:x_size,0:y_size]\n",
    "            # B02, B03, B04, B08 = B02[off:-off,off:-off], B03[off:-off,off:-off], B04[off:-off,off:-off], B08[off:-off,off:-off] \n",
    "            B02 = (B02-ch1_mean)/ch1_std\n",
    "            B03 = (B03-ch2_mean)/ch2_std\n",
    "            B04 = (B04-ch3_mean)/ch3_std\n",
    "            B08 = (B08-ch4_mean)/ch4_std\n",
    "            features = np.array([ B02, B03, B04, B08]).transpose(1,2,0)\n",
    "            # features = np.expand_dims(features, axis=0)\n",
    "            return features, label\n",
    "            # I have commented the line below but this should return\n",
    "            # the features and the label in a one hot vector\n",
    "            # return file['features'], file['targets']\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        exit(-1)\n",
    "        \n",
    "def data_loader(filename):\n",
    "    features, labels = tf.numpy_function(load_features, [filename], [tf.double, tf.int32])\n",
    "    # features.set_shape((None, 242,242,4))\n",
    "    # labels.set_shape(( 1))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f8d3b08-b09b-480d-8d8d-bf44131c2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/Users/sentinel_industry/downloaded_aois/coal/'\n",
    "\n",
    "pkl_files = glob.glob((path+\"*.pickle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7def905d-f2c1-45f1-985c-aaf123860dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2179\n"
     ]
    }
   ],
   "source": [
    "print(len(pkl_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f15f640f-7002-40d2-899a-21b4090c2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_steel = 'E:/Users/sentinel_industry/downloaded_aois/steel/'\n",
    "pkl_files_steel = glob.glob((path_steel+\"*.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e5a78e7-514d-4f65-a831-57faa0c5df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_steel_no_ind = 'E:/Users/sentinel_industry/downloaded_aois/steel/no_ind/'\n",
    "pkl_files_steel_no_ind = glob.glob((path_steel_no_ind+\"*.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "076b0a09-9866-4fee-a244-2bd75892bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coal_no_ind = 'E:/Users/sentinel_industry/downloaded_aois/coal/no_ind/'\n",
    "pkl_files_coal_no_ind = glob.glob((path_coal_no_ind+\"*.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da24ac7d-0f3f-4cfe-b518-fef29417d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1693\n"
     ]
    }
   ],
   "source": [
    "print(len(pkl_files_steel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af872199-9192-48eb-8cea-9693c2291fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_files = pkl_files + pkl_files_steel + pkl_files_steel_no_ind + pkl_files_coal_no_ind\n",
    "random.Random(42).shuffle(pkl_files)\n",
    "# pkl_files=pkl_files[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfe3ca4f-ca84-4d80-9218-95fc89e8fc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "\n",
    "# pkl_files = list(chain.from_iterable(zip(pkl_files, pkl_files_steel)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "07182a88-11f4-4b7d-a461-4debb2944063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7829\n"
     ]
    }
   ],
   "source": [
    "print(len(pkl_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfc46d5e-854e-42c6-af9c-97123ce4b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len=len(pkl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "037aad36-c73f-48bb-9c72-e4d8d4103687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21e3f5d5-620d-4e9d-8abb-ecee1ba2dac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices((pkl_files))\n",
    "\n",
    "# def data_loader(filename):\n",
    "#     features, labels = tf.numpy_function(load_features, [filename], [tf.double, tf.int32])\n",
    "#     # features.set_shape((None, 242,242,4))\n",
    "#     # labels.set_shape(( 1))\n",
    "#     return features, labels\n",
    "\n",
    "# dataset = dataset.map(data_loader)\n",
    "#                # load_features, [filename], [tf.uint16, tf.string])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "adaadc4a-c328-4c33-b9b7-3d2e140f92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len=len(pkl_files)\n",
    "\n",
    "train_size = int(np.floor(0.8 * dataset_len))\n",
    "valid_size = int(np.floor(0.1 * dataset_len))\n",
    "test_size = int(np.floor(0.1 * dataset_len))\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices(pkl_files[0:train_size]).map(data_loader,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid = tf.data.Dataset.from_tensor_slices(pkl_files[train_size:train_size+valid_size]).map(data_loader,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = tf.data.Dataset.from_tensor_slices(pkl_files[train_size+valid_size:]).map(data_loader,num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d59cee74-4666-4d78-802d-ce0537fb4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.experimental.save(\n",
    "#     valid, 'savedata_valid')\n",
    "\n",
    "# tf.data.experimental.save(\n",
    "#     train, 'savedata_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f35197-11ee-40af-bc9c-c5181c99583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = tf.data.experimental.load('savedata_valid')\n",
    "train = tf.data.experimental.load('savedata_train')\n",
    "train = train.shuffle(100,  reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676ae2c1-03cd-42bf-b40d-fe2926f9f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = 230-2*off\n",
    "y_s = 230-2*off\n",
    "in_seed = 42\n",
    "resize_rescale_augment = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.2, seed=in_seed, input_shape=(x_s,y_s,4)),  # here, maybe try larger values\n",
    "    layers.CenterCrop(140,140, ),\n",
    "    layers.RandomCrop(120,120, seed=in_seed, ),  # to be changed with centercrop for data augmentation/ use centre first, then random?\n",
    "    \n",
    "    layers.Rescaling(255),#,input_shape=(240,240,4)), # I think the data is scaled to 10000\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", seed=in_seed), \n",
    "    # norm_layer,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb171cdb-c5d8-4f7d-b778-18aa7efc08bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_6 (Sequential)   (None, 120, 120, 4)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 120, 120, 3)       303       \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 4, 4, 1280)       4049571   \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 20480)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 61443     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,111,317\n",
      "Trainable params: 4,069,294\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "313/313 [==============================] - 104s 293ms/step - loss: 1.5481 - categorical_accuracy: 0.5427 - val_loss: 1.1594 - val_categorical_accuracy: 0.5233 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 96s 289ms/step - loss: 1.0766 - categorical_accuracy: 0.6435 - val_loss: 1.5673 - val_categorical_accuracy: 0.6467 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 90s 282ms/step - loss: 0.9296 - categorical_accuracy: 0.6751 - val_loss: 1.2160 - val_categorical_accuracy: 0.6733 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 93s 293ms/step - loss: 0.9037 - categorical_accuracy: 0.6740 - val_loss: 0.8283 - val_categorical_accuracy: 0.7067 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 95s 298ms/step - loss: 0.8378 - categorical_accuracy: 0.6850 - val_loss: 0.9463 - val_categorical_accuracy: 0.6867 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 89s 277ms/step - loss: 0.7656 - categorical_accuracy: 0.7030 - val_loss: 0.6027 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 97s 305ms/step - loss: 0.6728 - categorical_accuracy: 0.7270 - val_loss: 0.5933 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 90s 282ms/step - loss: 0.6218 - categorical_accuracy: 0.7399 - val_loss: 0.5688 - val_categorical_accuracy: 0.7533 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 89s 280ms/step - loss: 0.6016 - categorical_accuracy: 0.7431 - val_loss: 0.5704 - val_categorical_accuracy: 0.7467 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 88s 277ms/step - loss: 0.5807 - categorical_accuracy: 0.7597 - val_loss: 0.6328 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 92s 291ms/step - loss: 0.5530 - categorical_accuracy: 0.7687 - val_loss: 0.5266 - val_categorical_accuracy: 0.7633 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 93s 292ms/step - loss: 0.5631 - categorical_accuracy: 0.7639 - val_loss: 0.6019 - val_categorical_accuracy: 0.7467 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 88s 278ms/step - loss: 0.6100 - categorical_accuracy: 0.7409 - val_loss: 0.5722 - val_categorical_accuracy: 0.7467 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 89s 277ms/step - loss: 0.5557 - categorical_accuracy: 0.7666 - val_loss: 0.5490 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 90s 282ms/step - loss: 0.5418 - categorical_accuracy: 0.7764 - val_loss: 0.5196 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 93s 291ms/step - loss: 0.5554 - categorical_accuracy: 0.7647 - val_loss: 0.5273 - val_categorical_accuracy: 0.7667 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 89s 278ms/step - loss: 0.5616 - categorical_accuracy: 0.7660 - val_loss: 0.5494 - val_categorical_accuracy: 0.7767 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 88s 275ms/step - loss: 0.5302 - categorical_accuracy: 0.7875 - val_loss: 0.5163 - val_categorical_accuracy: 0.7833 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 78s 245ms/step - loss: 0.5133 - categorical_accuracy: 0.7850 - val_loss: 0.4968 - val_categorical_accuracy: 0.7967 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 64s 201ms/step - loss: 0.5080 - categorical_accuracy: 0.7939 - val_loss: 0.4697 - val_categorical_accuracy: 0.8133 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 64s 202ms/step - loss: 0.5107 - categorical_accuracy: 0.7931 - val_loss: 0.5709 - val_categorical_accuracy: 0.7433 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 0.4687 - categorical_accuracy: 0.8101 - val_loss: 0.6902 - val_categorical_accuracy: 0.7133 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 63s 198ms/step - loss: 0.4925 - categorical_accuracy: 0.7982 - val_loss: 0.8109 - val_categorical_accuracy: 0.6400 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 0.7260 - categorical_accuracy: 0.7184 - val_loss: 0.8833 - val_categorical_accuracy: 0.6333 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 65s 205ms/step - loss: 0.9277 - categorical_accuracy: 0.6196 - val_loss: 1.4134 - val_categorical_accuracy: 0.5967 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 66s 207ms/step - loss: 0.7362 - categorical_accuracy: 0.6780 - val_loss: 0.6391 - val_categorical_accuracy: 0.7233 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 64s 202ms/step - loss: 0.6777 - categorical_accuracy: 0.7173 - val_loss: 0.6129 - val_categorical_accuracy: 0.7233 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.6709 - categorical_accuracy: 0.7157 - val_loss: 0.6308 - val_categorical_accuracy: 0.7333 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 0.7945 - categorical_accuracy: 0.6573 - val_loss: 0.8060 - val_categorical_accuracy: 0.6367 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 63s 201ms/step - loss: 0.7439 - categorical_accuracy: 0.6704 - val_loss: 0.9004 - val_categorical_accuracy: 0.5767 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 63s 199ms/step - loss: 0.7147 - categorical_accuracy: 0.6928 - val_loss: 0.6568 - val_categorical_accuracy: 0.7100 - lr: 9.7045e-04\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 0.6461 - categorical_accuracy: 0.7264 - val_loss: 0.6261 - val_categorical_accuracy: 0.7267 - lr: 9.4176e-04\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 62s 198ms/step - loss: 0.6354 - categorical_accuracy: 0.7347 - val_loss: 0.6135 - val_categorical_accuracy: 0.7400 - lr: 9.1393e-04\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.6648 - categorical_accuracy: 0.7224 - val_loss: 0.6962 - val_categorical_accuracy: 0.6900 - lr: 8.8692e-04\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 61s 193ms/step - loss: 0.6728 - categorical_accuracy: 0.7168 - val_loss: 0.5899 - val_categorical_accuracy: 0.7400 - lr: 8.6071e-04\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 60s 189ms/step - loss: 0.6363 - categorical_accuracy: 0.7433 - val_loss: 0.6002 - val_categorical_accuracy: 0.7233 - lr: 8.3527e-04\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 60s 189ms/step - loss: 0.6153 - categorical_accuracy: 0.7468 - val_loss: 0.6055 - val_categorical_accuracy: 0.7433 - lr: 8.1058e-04\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 62s 196ms/step - loss: 0.5862 - categorical_accuracy: 0.7625 - val_loss: 0.5498 - val_categorical_accuracy: 0.7733 - lr: 7.8663e-04\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.5693 - categorical_accuracy: 0.7652 - val_loss: 0.5385 - val_categorical_accuracy: 0.7767 - lr: 7.6338e-04\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.5726 - categorical_accuracy: 0.7612 - val_loss: 0.5757 - val_categorical_accuracy: 0.7667 - lr: 7.4082e-04\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 62s 198ms/step - loss: 0.5737 - categorical_accuracy: 0.7701 - val_loss: 0.5517 - val_categorical_accuracy: 0.7667 - lr: 7.1892e-04\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 63s 198ms/step - loss: 0.5699 - categorical_accuracy: 0.7722 - val_loss: 0.5574 - val_categorical_accuracy: 0.7600 - lr: 6.9768e-04\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 0.5544 - categorical_accuracy: 0.7717 - val_loss: 0.5168 - val_categorical_accuracy: 0.7933 - lr: 6.7706e-04\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.5468 - categorical_accuracy: 0.7768 - val_loss: 0.5692 - val_categorical_accuracy: 0.7467 - lr: 6.5705e-04\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 62s 198ms/step - loss: 0.5897 - categorical_accuracy: 0.7605 - val_loss: 0.5303 - val_categorical_accuracy: 0.7833 - lr: 6.3763e-04\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 62s 198ms/step - loss: 0.5466 - categorical_accuracy: 0.7733 - val_loss: 0.4922 - val_categorical_accuracy: 0.8067 - lr: 6.1878e-04\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 63s 198ms/step - loss: 0.6126 - categorical_accuracy: 0.7551 - val_loss: 0.6233 - val_categorical_accuracy: 0.7233 - lr: 6.0050e-04\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 0.5924 - categorical_accuracy: 0.7438 - val_loss: 0.5219 - val_categorical_accuracy: 0.7833 - lr: 5.8275e-04\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 62s 198ms/step - loss: 0.5550 - categorical_accuracy: 0.7657 - val_loss: 0.4925 - val_categorical_accuracy: 0.8000 - lr: 5.6553e-04\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 62s 196ms/step - loss: 0.5412 - categorical_accuracy: 0.7688 - val_loss: 0.4850 - val_categorical_accuracy: 0.8033 - lr: 5.4881e-04\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 62s 196ms/step - loss: 0.5261 - categorical_accuracy: 0.7800 - val_loss: 0.4679 - val_categorical_accuracy: 0.8200 - lr: 5.3259e-04\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 63s 201ms/step - loss: 0.5276 - categorical_accuracy: 0.7816 - val_loss: 0.4483 - val_categorical_accuracy: 0.8267 - lr: 5.1685e-04\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 63s 198ms/step - loss: 0.5108 - categorical_accuracy: 0.7890 - val_loss: 0.4370 - val_categorical_accuracy: 0.8267 - lr: 5.0158e-04\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 63s 197ms/step - loss: 0.5037 - categorical_accuracy: 0.7930 - val_loss: 0.4294 - val_categorical_accuracy: 0.8200 - lr: 4.8675e-04\n",
      "Epoch 55/200\n",
      "269/313 [========================>.....] - ETA: 7s - loss: 0.4707 - categorical_accuracy: 0.8069"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    resize_rescale_augment,\n",
    "    layers.Conv2D(3, 5, padding='same', activation='linear'), # this seems like a bit of a brute force approach to handing a 3 channel image to resnet, \n",
    "                                                            # maybe try changing the source so it accepts 4 channels?\n",
    "    tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=(120, 120, 3),\n",
    "    pooling=None,\n",
    "    classes=3),\n",
    "    layers.Flatten(), # does this make sense? or is there another way to get down to just three output dimensions?\n",
    "    layers.Dense(3)\n",
    "])\n",
    "\n",
    "log_dir = \"E:/Users/sentinel_industry/logs/train/threemod/effnet\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = '120,140')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 30:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.03)\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50,verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "# now with normalisation\n",
    "epochs=200\n",
    "batch_size = 20\n",
    "history = model.fit(\n",
    "    train.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE),\n",
    "    validation_data=valid.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE),\n",
    "    validation_steps=15,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tensorboard_callback, lr_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461561a4-bd73-4fd7-ad8b-84eaee27bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46647eff-f12c-4f55-bd21-09eb71452857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/resnet50v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/resnet50v2\\assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('saved_models/resnet50v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
